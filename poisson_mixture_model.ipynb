{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from typing import *\n",
    "\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各分布のハイパーパラメーターなどを保持するクラスを作る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gamma:\n",
    "    def __init__(self, *, a, b):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        \n",
    "    def get():\n",
    "        \"\"\" ガンマ分布をscipyのfrozen RV objectの形で返す。\n",
    "        Notes:\n",
    "            scipyのgamma distは、教科書の定義とちょっと異なる。教科書で言うハイパーパラメーターbは、\n",
    "            scaleという名前で指定する。\n",
    "            https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.gamma.html\n",
    "        \"\"\"\n",
    "        return [stats.gamma(a=elem, scale=b) for elem in self.a]\n",
    "\n",
    "\n",
    "class Poisson:\n",
    "    def __init__(self, *, lambda_: List[float]):\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def get():\n",
    "        \"\"\" ポアソン分布をscipyのfrozen RV objectの形で返す。\"\"\"\n",
    "        return [stats.poisson(mu=elem) for elem in self.lambda_]\n",
    "\n",
    "\n",
    "class BayesPoiMixModel:\n",
    "    def __init__(self, *, num_dim: int, num_cluster: int, alpha, gamma):\n",
    "        \"\"\" ポアソン混合モデルの事前・事後分布を表現するクラスを構築する。\n",
    "        Args:\n",
    "            D: 観測データの次元\n",
    "            K: クラスター数\n",
    "            alpha : カテゴリ分布のパラメーター $\\pi$ の共役事前分布であるディリクレ分布のハイパーパラメーター (p.119)\n",
    "            gamma : ポアソン分布のパラメーター $\\lambda$ の共役事前分布であるガンマ分布のハイパーパラメーター (p.129)\n",
    "                * 非リストを指定した場合、全クラスターに同じ値を指定する。\n",
    "                * リストを指定した場合、各クラスターにおのおの指定された値を指定する。ただし、len(gamma) != num_clusterの場合は例外を投げる。\n",
    "        \"\"\"\n",
    "        self.num_dim = num_dim\n",
    "        self.num_cluster = num_cluster\n",
    "        self.alpha = np.ones(num_cluster) * alpha\n",
    "        if isinstance(gamma, list):\n",
    "            if len(gamma) != num_cluster:\n",
    "                raise \"in BayesPoiMixModel.__init__() : len(gamma) != num_cluster\"\n",
    "            self.gamma = gamma\n",
    "        else: \n",
    "            self.gamma = [gamma]*num_cluster\n",
    "\n",
    "\n",
    "class PoiMixModel:\n",
    "    def __init__(self, *, num_dim: int, num_cluster: int, phi, poisson: Poisson):\n",
    "        \"\"\" 真のポアソン混合モデルを構築する。\n",
    "        Args:\n",
    "            D: 観測データの次元\n",
    "            K: クラスター数\n",
    "            alpha : \n",
    "            poiDists: \n",
    "        \"\"\"\n",
    "        self.num_dim = num_dim\n",
    "        self.num_cluster = num_cluster\n",
    "        self.phi = phi\n",
    "        if isinstance(poisson, list):\n",
    "            if len(poisson) != num_cluster:\n",
    "                raise \"in PoiMixModel.__init__() : len(poisson) != num_cluster\"\n",
    "            self.poisson = poisson\n",
    "        else: \n",
    "            self.poisson = [poisson]*num_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ユーティリティ関数を定義する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ELBO(X, prior: BayesPoiMixModel, posterior: BayesPoiMixModel): # X: Mat{float}\n",
    "    \"\"\" ELBOを計算する。付録A.4を参照のこと。\n",
    "    Args:\n",
    "        X: \n",
    "        prior: \n",
    "        posterior: \n",
    "    \"\"\"\n",
    "    ln_expt_S = update_S(pos, X)\n",
    "    expt_S = np.exp(ln_expt_S)\n",
    "    K, N = expt_S.shape\n",
    "    D = X.shape[0]\n",
    "\n",
    "    expt_ln_lambda = np.zeros(S.shape) #np.matrix(np.zeros(D, K))\n",
    "    expt_lambda = np.zeros(S.shape) #np.matrix(np.zeros(D, K))\n",
    "    expt_ln_lkh = 0\n",
    "    for k in range(K):\n",
    "        expt_ln_lambda[:,k] = np.digamma(pos.cmp[k].a) - np.log(pos.cmp[k].b)\n",
    "        expt_lambda[:,k] = pos.cmp[k].a / pos.cmp[k].b\n",
    "        for n in range(N):\n",
    "            expt_ln_lkh += expt_S[k,n] * (\n",
    "                X[:, n].T * expt_ln_lambda[:,k] \n",
    "                    - sum(expt_lambda[:,k]) - sum(np.lgamma(X[:,n]+1))\n",
    "            )[1]\n",
    "\n",
    "    expt_ln_pS = sum(expt_S.T * (np.digamma(pos.alpha) - np.digamma(sum(pos.alpha))))\n",
    "    expt_ln_qS = sum(expt_S * ln_expt_S)\n",
    "    KL_lambda = 0\n",
    "    for k in range(K):\n",
    "        KL_lambda += (\n",
    "            sum(pos.cmp[k].a)*np.log(pos.cmp[k].b) - sum(pri.cmp[k].a)*np.log(pri.cmp[k].b)\n",
    "                - sum(np.lgamma(pos.cmp[k].a)) + sum(np.lgamma(pri.cmp[k].a))\n",
    "                + (pos.cmp[k].a - pri.cmp[k].a).T * expt_ln_lambda[:,k]\n",
    "                + (pri.cmp[k].b - pos.cmp[k].b) * sum(expt_lambda[:,k])\n",
    "        )[1]\n",
    "    \n",
    "    KL_pi = (\n",
    "        np.lgamma(sum(pos.alpha)) - np.lgamma(sum(pri.alpha))\n",
    "             - sum(np.lgamma(pos.alpha)) + sum(np.lgamma(pri.alpha))\n",
    "             + (pos.alpha - pri.alpha).T * (np.digamma(pos.alpha) - np.digamma(sum(pos.alpha)))\n",
    "    )[1]\n",
    "    return expt_ln_lkh + expt_ln_pS - expt_ln_qS - (KL_lambda + KL_pi)\n",
    "\n",
    "\n",
    "def add_stats(bpmm: BayesPoiMixModel, X, S):  # X: Matrix{Float64}, S: Matrix{Float64}\n",
    "    D = bpmm.num_dim\n",
    "    K = bpmm.num_cluster\n",
    "    sum_S = sum(S, 2)\n",
    "    alpha = [bpmm.alpha[k] + sum_S[k] for k in range(K)]\n",
    "    gamDists = [] # Vector{Gam}()\n",
    "    XS = X @ S.T;\n",
    "    for k in range(K):\n",
    "        a = [float(bpmm.gamma[k].a[d] + XS[d,k]) for d in range(D)]\n",
    "        b = bpmm.gamma[k].b + sum_S[k]\n",
    "        gamDists.append(Gamma(a=a, b=b))\n",
    "    return BayesPoiMixModel(num_dim=D, num_cluster=K, alpha=alpha, gamma=gamDists)\n",
    "\n",
    "\n",
    "def remove_stats(bpmm: BayesPoiMixModel, X, S): # X: Matrix{Float64}, S: Matrix{Float64}\n",
    "    return add_stats(bpmm, X, -S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a PMM given hyperparameters.\n",
    "def sample_PMM(bpmm: BayesPoiMixModel) -> PoiMixModel:\n",
    "    pois = []\n",
    "    for c in bpmm.gamma:\n",
    "        lambda_ = np.zeros(bpmm.num_dim)\n",
    "        for d in range(bpmm.num_dim):\n",
    "            lambda_[d] = rnd.gamma(shape=c.a[d], scale=1.0/c.b)\n",
    "        pois.append(Poisson(lambda_ = lambda_))\n",
    "    return PoiMixModel(\n",
    "        num_dim = bpmm.num_dim, \n",
    "        num_cluster = bpmm.num_cluster,\n",
    "        phi = rnd.dirichlet(bpmm.alpha),\n",
    "        poisson=pois\n",
    "    )\n",
    "\n",
    "\n",
    "# Sample data from a specific PMM model.\n",
    "def sample_data(pmm: PoiMixModel, N: int):\n",
    "    x = np.zeros((pmm.num_dim, N)) #np.matrix(np.zeros(pmm.D, N))\n",
    "    s = categorical_sample(pmm.phi, N)\n",
    "    for n in range(N):\n",
    "        k = np.argmax(s[:, n])\n",
    "        for d in range(pmm.num_dim):\n",
    "            x[d, n] = rnd.poisson(pmm.poisson[k].lambda_[d])\n",
    "    return x, s\n",
    "\n",
    "#categorical_sample(p::Vector{Float64}) = categorical_sample(p, 1)[:,1]\n",
    "\n",
    "def categorical_sample(p: List[float], N: int = 1):\n",
    "    K = len(p)\n",
    "    S = np.zeros((K, N)) # np.matrix(np.zeros(K, N))\n",
    "    # numpy.randomにはわかりやすくcategorical sampleを行う関数がない。\n",
    "    # ググったところ、numpy.random.Generator.choice()がカテゴリ分布になっているらしい。\n",
    "    # http://bois.caltech.edu/distribution_explorer/discrete/categorical.html\n",
    "    S_tmp = np.random.default_rng().choice(len(p), N, p=p)\n",
    "    for k in range(K):\n",
    "        #S[S[k, S_tmp == k]] = 1\n",
    "        #S[k,find(S_tmp==k)] = 1\n",
    "        S[k, S_tmp == k] = 1\n",
    "    return S if N != 1 else S[:, 1]\n",
    "\n",
    "\n",
    "def init_S(x, bpmm: BayesPoiMixModel): # X: Matrix{float}\n",
    "    s = categorical_sample(np.ones(bpmm.num_cluster)/bpmm.num_cluster, x.shape[1])    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ギブスサンプリングを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for Gibbs Sampling\n",
    "def sample_S_GS(pmm: PoiMixModel, X):  # X: Matrix{Float64}\n",
    "    D, N = X.shape\n",
    "    K = pmm.num_cluster\n",
    "    S = np.zeros((K, N))\n",
    "\n",
    "    tmp = [-sum(pmm.poisson[k].lambda_) + np.log(pmm.phi[k]) for k in range(K)]\n",
    "    ln_lambda_X = [X.T*np.log(pmm.poisson[k].lambda_) for k in range(K)]\n",
    "    for n in range(N):\n",
    "        tmp_ln_phi = [(tmp[k] + ln_lambda_X[k][n]) for k in range(K)]\n",
    "        tmp_ln_phi = tmp_ln_phi - logsumexp(tmp_ln_phi)\n",
    "        S[:,n] = categorical_sample(np.exp(tmp_ln_phi))\n",
    "    return S\n",
    "\n",
    "\n",
    "def conduct_Gibbs_sampling(x, s, prior: BayesPoiMixModel):  # X::Matrix{Float64}\n",
    "    # sample parameters\n",
    "    pmm = sample_PMM(prior)\n",
    "    # sample latent variables\n",
    "    s = sample_S_GS(pmm, x)\n",
    "    # update current model\n",
    "    bpmm = add_stats(prior, x, s)\n",
    "    return s, bpmm, calc_ELBO(x, prior, bpmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "変分推論を実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior distribution via variational inference.\n",
    "def update_S(x, prior: BayesPoiMixModel):\n",
    "    D, N = size(X)\n",
    "    num_cluster = prior.num_cluster\n",
    "    ln_expt_S = np.zeros((K, N))\n",
    "    tmp = np.zeros(K)\n",
    "\n",
    "    sum_digamma_tmp = np.digamma(sum(bpmm.alpha))\n",
    "    for k in range(num_cluster):\n",
    "        tmp[k] = - sum(bpmm.cmp[k].a) / bpmm.cmp[k].b\n",
    "        tmp[k] += np.digamma(bpmm.alpha[k]) - sum_digamma_tmp\n",
    "    ln_lambda_X = [X.T*(np.digamma(bpmm.cmp[k].a) - np.log(bpmm.cmp[k].b)) for k in range(num_cluster)]\n",
    "    for n in range(N):\n",
    "        tmp_ln_pi =  [tmp[k] + ln_lambda_X[k][n] for k in range(K)]\n",
    "        ln_expt_S[:,n] = tmp_ln_pi - logsumexp(tmp_ln_pi)\n",
    "    return ln_expt_S\n",
    "\n",
    "\n",
    "def conduct_variational_inference(x, s, prior_bpmm: BayesPoiMixModel):  # X: Matrix{Float64}\n",
    "    \"\"\" 変分推論により事後分布を推定する。\n",
    "    \"\"\"\n",
    "    # E-step\n",
    "    expt_S = np.exp(update_S(bpmm, x))\n",
    "    # M-step\n",
    "    posterior = add_stats(prior, x, expt_S)\n",
    "    return expt_S, bpmm, calc_ELBO(x, prior, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "崩壊型ギブスサンプリングを実装する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used for Collapsed Gibbs Sampling\n",
    "def calc_ln_NB(Xn, gam: Gamma): # Xn: Vector{Float64}\n",
    "    return sum([\n",
    "        float(gam.a[d]*np.log(gam.b)\n",
    "            - np.lgamma(gam.a[d])\n",
    "            + np.lgamma(Xn[d] + gam.a[d])\n",
    "            - (Xn[d] + gam.a[d])*np.log(gam.b + 1)\n",
    "        ) for d in range(Xn.shape[0])\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_Sn(Xn, bpmm: BayesPoiMixModel): # Xn: Vector{Float64}\n",
    "    ln_tmp = [(calc_ln_NB(Xn, bpmm.cmp[k]) + np.log(bpmm.alpha[k])) for k in range(bpmm.K)]\n",
    "    ln_tmp = ln_tmp -  logsumexp(ln_tmp)\n",
    "    Sn = categorical_sample(np.exp(ln_tmp))\n",
    "    return Sn\n",
    "\n",
    "\n",
    "def sample_S_CGS(S, M, bpmm: BayesPoiMixModel):  # S: Matrix{Float64}, X: Matrix{Float64}\n",
    "    D, N = X.shape\n",
    "    K = S.shape[0]\n",
    "    for n in randperm(N):\n",
    "        bpmm = remove_stats(bpmm, X[:,[n]], S[:,[n]])  # remove\n",
    "        S[:,n] = sample_Sn(X[:,n], bpmm)  # sample\n",
    "        bpmm = add_stats(bpmm, X[:,[n]], S[:,[n]])  # insert\n",
    "    return S, bpmm\n",
    "\n",
    "\n",
    "def conduct_collapsed_Gibbs_sampling(x, s, prior: BayesPoiMixModel):  \n",
    "    # directly sample S\n",
    "    s, posterior = sample_S_CGS(s, x, prior)\n",
    "    return s, bpmm, calc_ELBO(x, prior, posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指定されたアルゴリズムをstep-by-stepに実行していくwrapperを作っておく。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    \"GS\": conduct_Gibbs_sampling, \n",
    "    \"VI\": conduct_variational_inference, \n",
    "    \"CGS\": conduct_collapsed_Gibbs_sampling,\n",
    "}\n",
    "\n",
    "\n",
    "def learn_step_by_step(x, s, prior: BayesPoiMixModel, algo: str):\n",
    "    return algorithms[algo](x, s, prior)\n",
    "\n",
    "\n",
    "class Cache:\n",
    "    def __init__(self, x, s, prior):\n",
    "        self.x = x\n",
    "        self.s = s\n",
    "        self.prior = prior\n",
    "\n",
    "\n",
    "def learn(x, s, prior: BayesPoiMixModel, iter_=1000):\n",
    "    # 初期化\n",
    "    s = init_S(x, prior)\n",
    "    prior = add_stats(prior, x, s)\n",
    "    \n",
    "    # 各アルゴリズムの結果をキャッシュしておく変数を作る\n",
    "    gs, vi, cgs = Cache(x, s, prior), Cache(x, s, prior), Cache(x, s, prior)\n",
    "    for i in range(iter_):\n",
    "        gs = learn_step_by_step(gs.x, gs.s, gs.prior, \"GS\")\n",
    "        vi = learn_step_by_step(vi.x, vi.s, vi.prior, \"VI\")\n",
    "        cgs = learn_step_by_step(cgs.x, cgs.s, cgs.prior, \"CGS\")\n",
    "    \n",
    "    return gs, vi, cgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデルを作って、クラスタリングを行うデータを見てみる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "prior_ = BayesPoiMixModel(\n",
    "    num_dim=1, \n",
    "    num_cluster=2, \n",
    "    alpha=100.0, \n",
    "    gamma=Gamma(a=1.0*np.ones(1), b=0.01)\n",
    ")\n",
    "\n",
    "pmm_ = sample_PMM(prior_)  # 真のモデルを作る\n",
    "x_, s_ = sample_data(pmm_, 1000)  # データを作る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタリングを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logsumexp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-54a41deb7847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-c7532e43b817>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(x, s, prior, iter_)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_step_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mvi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_step_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"VI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mcgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn_step_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CGS\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-c7532e43b817>\u001b[0m in \u001b[0;36mlearn_step_by_step\u001b[0;34m(x, s, prior, algo)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlearn_step_by_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBayesPoiMixModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-92cc45d1026b>\u001b[0m in \u001b[0;36mconduct_Gibbs_sampling\u001b[0;34m(x, s, prior)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_PMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# sample latent variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_S_GS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;31m# update current model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbpmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-fbad5ac201a7>\u001b[0m in \u001b[0;36msample_S_GS\u001b[0;34m(pmm, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtmp_ln_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mln_lambda_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtmp_ln_phi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_ln_phi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_ln_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorical_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_ln_phi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logsumexp' is not defined"
     ]
    }
   ],
   "source": [
    "results = learn(x_, s_, prior_, iter_=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
